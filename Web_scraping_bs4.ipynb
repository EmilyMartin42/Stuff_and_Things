{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "announced-recruitment",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "## An introduction using Beautiful Soup\n",
    "\n",
    "This lesson will introduce you to the world of web scraping and show you how one method, Beautiful Soup, works to allow you to gather large amounts of data from all sorts of web pages. Basic HTML structure will also be introduced and you will learn how to navigate the \"tree\" structure. \n",
    " \n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 45 Minutes\n",
    "\n",
    "**Knowledge Required:** Python basics (variables, data types, functions, loops, list comprehensions), Pandas basics\n",
    "\n",
    "**Knowledge Recommended:** HTML/webpage structure basic knowledge\n",
    "\n",
    "**Learning Objectives:** After this lesson you will be able to:\n",
    "1. Understand and implement web scraping using BeautifulSoup.\n",
    "2. Identify and navigate HTML tree structure to isolate desired nodes.\n",
    "3. Build the first steps of a text analysis pipeline.\n",
    "4. Be aware of other resources and methods for web scraping.\n",
    "\n",
    "**Research Pipeline:** \n",
    "1. Choose your resources - website(s) and volume of desired content\n",
    "2. Use the skills in this lesson to obtain the data and turn it into a user-friendly dataframe \n",
    "3. Transform the data as best fits your project goals - once it is in a data frame many things are possible!\n",
    "\n",
    "**Libraries Used:** \n",
    "- [Pandas](https://pandas.pydata.org) _For manipulating and organizing data_\n",
    "- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) _For web scraping_   \n",
    "- [Requests](https://pypi.org/project/requests/) _For accessing webpages_   \n",
    "- [Urllib](https://pypi.org/project/urllib3/) _As an HTTP client_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-lawrence",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**What is web scraping?**\n",
    "- Web scraping refers to the process of extracting content (data) from a webpage using some form of software (bot, web crawler). \n",
    "- Utilizes the structure of web pages (which are built using HTML or XHTML, a text-based mark-up language) to extract the data you want.\n",
    "\n",
    "**Why should I learn web scraping?**\n",
    "- While there are many pre-compiled datasets out there, many projects require specific data that is not readily available. Being able to gather the precise data you want for your project is a huge advantage.\n",
    "- Web scraping allows you to harvest very large amounts of data with ease.\n",
    "\n",
    "**How do I learn web scraping?**\n",
    "- There are many methods for web scraping, with some being more optimized for obtaining certain information. You can build your own, as we do in this lesson with Beautiful Soup, which allows you to obtain very specific data but must be modified for different sources. There is also pre-built software you can download, which can require little to no coding. This can be a browser extension or software run through the command line or through a user interface. \n",
    "- In this lesson we will focus on one particular method, but it is by no means the only option out there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-insulation",
   "metadata": {},
   "source": [
    "## HTML, a brief overview\n",
    "\n",
    "Before we get into web scraping we first need to understand what goes on behind the scenes of a webpage. Webpages are built from HTML or XHTML, which is a text based markup language. There is a hierarchy to HTML and once you understand how it works traversing it to find the information you need becomes simple.\n",
    "\n",
    "### The hierarchy\n",
    "\n",
    "The structure of HTML is often referred to as a tree because it is made up of one root element with any number of branching descendant nodes. For instance, a root `<html>` node may have two children, a `<head>` node and a `<body>` node; each of which contains its own children and their children. Therefore, the node directly below another in the tree structure is a child node while the entirety of its children and its childrens children are descendants of that node. Children can also have parents (the node above them) and siblings (those nodes on the same level as them). Thinking of it in these familial terms can make it more intuitive. Below is an image that illustrates this tree structure: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-budget",
   "metadata": {},
   "source": [
    "<img src='https://www.dropbox.com/home/Ithaka%20web%20scraping%20images?preview=treeStructure.png' alt=\"The hierarchical tree structure of HTML\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-berry",
   "metadata": {},
   "source": [
    "![ugh](/Independent Learning/treeStructure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-banana",
   "metadata": {},
   "source": [
    "Simply put, elements are pairs of tags which tell the browser what information is stored between them and how to display it. For instance, `<p>` elements are used to contain paragraphs (textual data) and `<a>` elements contain links. There are many kinds of tags, but for our purposes most of the information we are looking for will be in `<p>`, `<a>` or `<span>` tags, which are typically children of `<div>` tags. You may notice that there are also words like `class` or `id` within these tags that have their own values. These may look confusing at first, but they are very helpful in helping to point to the exact node you want. \n",
    "\n",
    "### Finding the information you need\n",
    "\n",
    "But how do you go about finding the node you need? In order to do this you need to do a bit of digging and leg work. To view this underlying structure you can navigate to the webpage you wish to scrape and hit `CTRL + right click` and then choose 'Inspect' or 'Inspect Element'. This will pull up a side or bottom panel that provides an interactive view of the underlying HTML. There is often an overwelming number of tags so it can be helpful to highlight a word in the main text you wish to obtain and then hitting `CTRL + right click` as this will take you directly to that location in the hierarchy. From here you can see the name of the element that contains the information you want along with its parent, which is the information you will need for scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-bibliography",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "raised-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-stream",
   "metadata": {},
   "source": [
    "## Scraping: Getting Started\n",
    "\n",
    "In this lesson we are going to scrape the Pitt News website and gather the articles that are currently on the home page. You can visit their website [here](https://pittnews.com/). We are going to begin by navigating to the home page and collecting all the links to the articles that exist there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accurate-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the URL in a variable\n",
    "baseUrl = 'https://pittnews.com/'\n",
    "\n",
    "# Initialize an empty list for the links\n",
    "hrefList = []\n",
    "\n",
    "# Call the base URL, turn it into soup then find all parents of link elements, \n",
    "##  drill down to the children and append each link to the list\n",
    "response = requests.get(baseUrl)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "divs = soup.find_all('div', {'class':'sno-story-card-link'})\n",
    "for div in divs:\n",
    "    a = div.find('a')\n",
    "    hrefList.append(a.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neural-salem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "https://pittnews.com/article/174298/news/not-on-campus-on-another-campus-about-130-pitt-first-years-to-live-in-carlow-university-dorms-this-academic-year/\n"
     ]
    }
   ],
   "source": [
    "# Check that it did what we think\n",
    "print(len(hrefList))\n",
    "print(hrefList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-archive",
   "metadata": {},
   "source": [
    "### Scraping: Gathering Data\n",
    "\n",
    "This chunk of code is a little more dense, so here is a brief explanation. Three empty dictionaries are initialized, one for the title of each article, one for the date it was published and one for the text of the article. Then we loop through each link from the list created above and open and collect the html from each page. This is turned to soup and from there each element we need is found and turned to text; which is then added as the value to its respective dictionary with the link to the article as the key. Using the links as keys allows us to utilize a very useful function later on when we are building the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "million-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text and other details from articles\n",
    "titleDict = {}\n",
    "dateDict = {}\n",
    "textDict = {}\n",
    "\n",
    "for href in hrefList:\n",
    "    response = requests.get(href)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    try:\n",
    "        # Get the date/time for each article\n",
    "        date = soup.find('span', {'class':'time-wrapper'}).text\n",
    "        dateDict[href] = date\n",
    "        \n",
    "        # Get the text for each article\n",
    "        content = soup.find('span', {'class':'storycontent'})\n",
    "        paras = content.find_all('p')\n",
    "        text = [t.text for t in paras]\n",
    "        textDict[href] = text\n",
    "        \n",
    "        # Get the title of each article\n",
    "        title = soup.find('h1', {'class':'storyheadline'}).text\n",
    "        titleDict[href] = title\n",
    "    except:\n",
    "        print('There is an error with this link')  # So we know if there is an issue with a link\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reasonable-cricket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https://pittnews.com/article/174298/news/not-on-campus-on-another-campus-about-130-pitt-first-years-to-live-in-carlow-university-dorms-this-academic-year/', '‘Not on campus, on another campus’: About 130 Pitt first-years to live in Carlow University dorms this academic year')\n",
      "('https://pittnews.com/article/174298/news/not-on-campus-on-another-campus-about-130-pitt-first-years-to-live-in-carlow-university-dorms-this-academic-year/', ' August 25, 2022')\n",
      "['htt', ['Ah, Pittsburgh — it has us Panthers sweating buckets in sweltering heat one minute, then running to take cover from a torrential downpour the next. As fall semester kicks off, it’s just a matter of time before we’re walking to our classes in blistering cold.', 'Fall is the best season to spend as much time outside as possible. We’re fortunate enough to have some beautiful natural spots on our mostly urban campus, where we can enjoy some foliage in not-too-torturous temperatures.', 'With that in mind, this is a guide to some of the best outdoor study spaces on campus — from a student who prefers to work with a little fresh air and white background noise.']]\n"
     ]
    }
   ],
   "source": [
    "# Checking our collected data\n",
    "# Turning it into a list first because dictionaries are not iterable\n",
    "\n",
    "titleList = list(titleDict.items())\n",
    "print(titleList[0])\n",
    "\n",
    "dateList = list(dateDict.items()) \n",
    "print(dateList[0])\n",
    "\n",
    "textList = list(textDict.items())\n",
    "print([t[:3] for t in textList[7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-corner",
   "metadata": {},
   "source": [
    "### Scraping: Cleaning the Data\n",
    "\n",
    "Now that we have the data we want and have taken a quick look at it we are can there are a few things that need to be taken care of before the data is ready to be stored in a more final form. Firstly there are some strange special characters in the text (`\\xa0`) which we want to remove. You may also have noticed that the text data is a list of strings, not one cohesive string. These strings should be joined into one before we move forward. \n",
    "\n",
    "This data is fairly clean, however this part of the data scraping pipeline can vary depending on the state of your data and the goal of your analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alpha-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of the special character\n",
    "for k,v in textDict.items():\n",
    "    vrep = [x.replace('\\xa0', '') for x in v]\n",
    "    textDict[k] = vrep\n",
    "\n",
    "# Joining the sentences \n",
    "textListCleaned = [' '.join(v) for k,v in textDict.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-communications",
   "metadata": {},
   "source": [
    "### Scraping: Storing the Data\n",
    "\n",
    "Now that the data is cleaned it can go in a data frame for futher manipulation or storage - whatever your project requires! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "integrated-london",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Href</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date/Time</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://pittnews.com/article/174298/news/not-o...</td>\n",
       "      <td>‘Not on campus, on another campus’: About 130 ...</td>\n",
       "      <td>August 25, 2022</td>\n",
       "      <td>Noah Stephenson has a few complaints about his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://pittnews.com/article/174347/sports/to-...</td>\n",
       "      <td>‘To finish first, first you must finish’: Pant...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>On any given day during the school year, sever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://pittnews.com/article/174308/news/bank-...</td>\n",
       "      <td>Bank accounts, dining and more: SGB members ta...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>As the fall semester begins, Pitt’s Student Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://pittnews.com/article/174339/sports/col...</td>\n",
       "      <td>Column | No need to worry about Pitt’s wide re...</td>\n",
       "      <td>August 25, 2022</td>\n",
       "      <td>Replacing a starting wide receiver is a diffic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://pittnews.com/article/174349/opinions/o...</td>\n",
       "      <td>Opinion | Your education is a privilege, not a...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>Most university students know that feeling whe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Href  \\\n",
       "0  https://pittnews.com/article/174298/news/not-o...   \n",
       "1  https://pittnews.com/article/174347/sports/to-...   \n",
       "2  https://pittnews.com/article/174308/news/bank-...   \n",
       "3  https://pittnews.com/article/174339/sports/col...   \n",
       "4  https://pittnews.com/article/174349/opinions/o...   \n",
       "\n",
       "                                               Title         Date/Time  \\\n",
       "0  ‘Not on campus, on another campus’: About 130 ...   August 25, 2022   \n",
       "1  ‘To finish first, first you must finish’: Pant...   August 26, 2022   \n",
       "2  Bank accounts, dining and more: SGB members ta...   August 26, 2022   \n",
       "3  Column | No need to worry about Pitt’s wide re...   August 25, 2022   \n",
       "4  Opinion | Your education is a privilege, not a...   August 26, 2022   \n",
       "\n",
       "                                                Text  \n",
       "0  Noah Stephenson has a few complaints about his...  \n",
       "1  On any given day during the school year, sever...  \n",
       "2  As the fall semester begins, Pitt’s Student Go...  \n",
       "3  Replacing a starting wide receiver is a diffic...  \n",
       "4  Most university students know that feeling whe...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Title dictionary to dataframe\n",
    "pittNewsDF = pd.DataFrame.from_dict(titleDict, orient='index')\n",
    "# Reset the index so it is not the hrefs\n",
    "pittNewsDF.reset_index(inplace=True)\n",
    "# Name the columns\n",
    "pittNewsDF.columns = ['Href', 'Title']\n",
    "# Add our other dictionaries to the data frame using zip()\n",
    "pittNewsDF['Date/Time'] = pittNewsDF['Href'].map(dateDict)\n",
    "pittNewsDF['Text'] = textListCleaned\n",
    "# Take a look\n",
    "pittNewsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incoming-pathology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40 entries, 0 to 39\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Href       40 non-null     object\n",
      " 1   Title      40 non-null     object\n",
      " 2   Date/Time  40 non-null     object\n",
      " 3   Text       40 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Taking a quick look at the data frame\n",
    "pittNewsDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-joseph",
   "metadata": {},
   "source": [
    "## Caveats and Future Directions\n",
    "\n",
    "Congratulations! You now know the basics of web scraping and the mechanics of one scraping method. Armed with this knowledge, and perhaps a little documentation study, you are ready to tackle any website you wish and gather whatever data you need. However, before we get too carried away it is important to note that while Beautiful Soup is powerful there are some things it is not equipt to handle. \n",
    "\n",
    "**The Caveats**\n",
    "- Beautiful Soup cannot handle JavaScript.\n",
    "- It has to be tweaked for every new website, which does require some hands on work initially.\n",
    "- Depends on having a parser and a library to send requests.\n",
    "- Can be laggy and bad for huge quantities of data.\n",
    "- Doesn't have good proxy support, so your IP address could be flagged.\n",
    "\n",
    "**Some Other Options**\n",
    "\n",
    "Since Beautiful Soup cannot handle everything (and maybe it is just not your favorite web scraping method) here are some other options that you might want to look into:\n",
    "- [Selenium](https://www.selenium.dev): is a popular method that is able to handle JavaScript and AJAX requests and is very beginner friendly.\n",
    "- [Scrapy](https://scrapy.org): an open source framework that is excellent at handling proxies and building pipelines for very large quantities of data. \n",
    "\n",
    "**Some Useful Resources**\n",
    "\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-grounds",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
